{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hse_pix2pix.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HSESPBPIX2PIX/hse_spb_pix2pix/blob/main/hse_pix2pix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IfX1NnnbwxY"
      },
      "source": [
        "!unzip -qq Cats.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy529orSb2Kd"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClRGoiwQb4_1"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVUNoexBb7Xm"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "\n",
        "class ImageLoader():\n",
        "    def __init__(self, transform=None):\n",
        "        self.transform = transform\n",
        "\n",
        "    def load(self, path):\n",
        "        out = Image.open(path).convert('RGB')\n",
        "\n",
        "        if self.transform is not None:\n",
        "            out = self.transform(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGFgNCwyb-kp"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root, load_percent=1, padding=0, transform=None):\n",
        "        self.transform = transform\n",
        "        self.loader = ImageLoader(transform)\n",
        "        \n",
        "        self.paths = self.get_paths(root)\n",
        "        \n",
        "        start = int(len(self.paths) * padding)\n",
        "        end = start + int(len(self.paths) * load_percent)\n",
        "        self.paths = self.paths[start:end]\n",
        "        \n",
        "    def get_paths(self, root):\n",
        "        path = os.path.join(root, '*')\n",
        "        paths = glob.glob(path)\n",
        "        \n",
        "        exts = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n",
        "        paths = filter(lambda p: p.endswith(exts), paths)\n",
        "        paths = list(paths)\n",
        "        \n",
        "        return paths\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path = self.paths[index]\n",
        "        img = self.loader.load(path)\n",
        "        return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkD2oB8icCOq"
      },
      "source": [
        "import cv2 as cv\n",
        "\n",
        "\n",
        "def tensor_to_cv(x):\n",
        "    x = x.detach().cpu().numpy()\n",
        "    x = (x + 1) / 2\n",
        "    x = x.transpose(1, 2, 0)\n",
        "\n",
        "    x = (x * 255).astype(np.uint8)\n",
        "    x = cv.cvtColor(x, cv.COLOR_BGR2GRAY)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def cv_to_tensor(x):\n",
        "    x = x / 255.0\n",
        "\n",
        "    x = x[None, :, :]\n",
        "    x = 2 * x - 1\n",
        "    x = torch.FloatTensor(x)\n",
        "    \n",
        "    x = x.view((x.shape[0], 1, *x.shape[1:]))\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def auto_canny(image, sigma=0.33):\n",
        "    v = np.median(image)\n",
        "    lower = int(max(0, (1.0 - sigma) * v))\n",
        "    upper = int(min(255, (1.0 + sigma) * v))\n",
        "    \n",
        "    edged = cv.Canny(image, lower, upper)\n",
        "\n",
        "    return edged\n",
        "\n",
        "\n",
        "def to_edge(x):\n",
        "#     return to_bw(x)\n",
        "\n",
        "    device = x.device\n",
        "\n",
        "    x = tensor_to_cv(x)\n",
        "    x = cv.blur(x, (5, 5))\n",
        "    x = auto_canny(x)\n",
        "    x = cv_to_tensor(x)\n",
        "    x = x.to(device)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def to_bw(x):\n",
        "    device = x.device\n",
        "    \n",
        "    x = tensor_to_cv(x)\n",
        "    x = cv.threshold(x, 120, 255, cv.THRESH_BINARY)[1]\n",
        "    x = cv_to_tensor(x)\n",
        "    x = x.to(device)\n",
        "    \n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwALAKsScFfh"
      },
      "source": [
        "class EdgeDatasetWrapper(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.dataset = dataset\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.dataset[index]\n",
        "        x_edge = to_edge(x)\n",
        "        return x, x_edge\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rp0YfKIvcI6c"
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "def get_transform(image_size, aug=False):\n",
        "    if aug:\n",
        "        bigger_image_size = (image_size // 8 + 1) * 8\n",
        "        ts = [\n",
        "            transforms.Resize((bigger_image_size, bigger_image_size)),\n",
        "            transforms.RandomResizedCrop((image_size, image_size)),\n",
        "            transforms.RandomHorizontalFlip()\n",
        "        ]\n",
        "    else:\n",
        "        ts = [transforms.Resize((image_size, image_size))]\n",
        "        \n",
        "    ts += [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ]\n",
        "    \n",
        "    return transforms.Compose(ts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_X4EysYFcLOP"
      },
      "source": [
        "image_size = 256\n",
        "\n",
        "train_transforms = get_transform(image_size, aug=True)\n",
        "train_dataset = ImageDataset(root='./Cats', load_percent=0.9, transform=train_transforms)\n",
        "train_dataset = EdgeDatasetWrapper(train_dataset)\n",
        "\n",
        "val_transforms = get_transform(image_size, aug=False)\n",
        "val_dataset = ImageDataset(root='./Cats', padding=0.9, load_percent=0.1, transform=val_transforms)\n",
        "val_dataset = EdgeDatasetWrapper(val_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfJV4q34cbdk"
      },
      "source": [
        "def collate_fn(data):\n",
        "    x1, x2 = zip(*data)\n",
        "\n",
        "    x1 = torch.stack(x1, dim=0)\n",
        "    x2 = torch.stack(x2, dim=0)\n",
        "\n",
        "    return x1, x2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUJ29cCZcfXb"
      },
      "source": [
        "len(val_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0rwUh2xciQg"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "batch_size = 1\n",
        "\n",
        "\n",
        "trainloader = DataLoader(\n",
        "    dataset=train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=7,\n",
        "    collate_fn=collate_fn,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "valloader = DataLoader(\n",
        "    dataset=val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=7,\n",
        "    collate_fn=collate_fn,\n",
        "    drop_last=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jX0m5sFMckkt"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8GlPb0jcm-j"
      },
      "source": [
        "import wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sqkjMeycpmr"
      },
      "source": [
        "wandb.login(key='')  #input your API key"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FT5F5tUcyIk"
      },
      "source": [
        "from torch import nn\n",
        "from torch.nn import Sequential as Seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu1tYy6pc0z6"
      },
      "source": [
        "def conv1x1(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=1,\n",
        "                     stride=stride)\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3,\n",
        "                     stride=stride, padding=1)\n",
        "\n",
        "def conv4x4(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=4,\n",
        "                     stride=stride, padding=1)\n",
        "\n",
        "def conv5x5(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=5,\n",
        "                     stride=stride, padding=2)\n",
        "\n",
        "def conv7x7(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=7,\n",
        "                     stride=stride, padding=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaWeM36-c3RU"
      },
      "source": [
        "def conv_by_name(kernel_size):\n",
        "    if kernel_size == '1x1':\n",
        "        return conv1x1\n",
        "    elif kernel_size == '3x3':\n",
        "        return conv3x3\n",
        "    elif kernel_size == '4x4':\n",
        "        return conv4x4\n",
        "    elif kernel_size == '5x5':\n",
        "        return conv5x5\n",
        "    elif kernel_size == '7x7':\n",
        "        return conv7x7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP9nPiOac5mx"
      },
      "source": [
        "def activ_by_name(name):\n",
        "    if name == 'relu':\n",
        "        return nn.ReLU6()\n",
        "    elif name == 'lerelu':\n",
        "        return nn.LeakyReLU(0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rai-pThrc8pT"
      },
      "source": [
        "def transp1x1(in_planes, out_planes, stride=1):\n",
        "    return nn.ConvTranspose2d(in_planes, out_planes, kernel_size=1,\n",
        "                     stride=stride)\n",
        "\n",
        "def transp3x3(in_planes, out_planes, stride=1):\n",
        "    return nn.ConvTranspose2d(in_planes, out_planes, kernel_size=2,\n",
        "                     stride=stride, padding=0)\n",
        "\n",
        "def transp4x4(in_planes, out_planes, stride=1):\n",
        "    return nn.ConvTranspose2d(in_planes, out_planes, kernel_size=4,\n",
        "                     stride=stride, padding=0)\n",
        "\n",
        "def transp5x5(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=5,\n",
        "                     stride=stride, padding=0)\n",
        "\n",
        "def transp7x7(in_planes, out_planes, stride=1):\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=7,\n",
        "                     stride=stride, padding=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNqLahx_c_L-"
      },
      "source": [
        "def tranpose_by_name(kernel_size):\n",
        "    if kernel_size == '1x1':\n",
        "        return transp1x1\n",
        "    elif kernel_size == '3x3':\n",
        "        return transp3x3\n",
        "    elif kernel_size == '4x4':\n",
        "        return transp4x4\n",
        "    elif kernel_size == '5x5':\n",
        "        return transp5x5\n",
        "    elif kernel_size == '7x7':\n",
        "        return transp7x7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajZSrVMvdBat"
      },
      "source": [
        "class ResCompressBlock(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, activ_name='relu'):\n",
        "        super(ResCompressBlock, self).__init__()\n",
        "        self.layer1 = Seq(\n",
        "            conv_by_name(kernel_size)(in_channels, out_channels, stride),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            activ_by_name(activ_name),\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        identity = x\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        \n",
        "        x = torch.cat(identity, x, dim=1)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ygpak772dDpJ"
      },
      "source": [
        "class CompressBlock(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, kernel_size, apply_batchnorm=True, stride=2, activ_name='lerelu'):\n",
        "        super(CompressBlock, self).__init__()\n",
        "        self.layer1 = Seq()\n",
        "        self.layer1.add_module('conv', conv_by_name(kernel_size)(in_channels, out_channels, stride)),\n",
        "        if apply_batchnorm:\n",
        "            self.layer1.add_module('batch', nn.BatchNorm2d(out_channels)),\n",
        "        self.layer1.add_module('relu', activ_by_name(activ_name)),\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhoAwnMvdGBR"
      },
      "source": [
        "class ExpandBlock(nn.Module):\n",
        "    \n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=2, apply_dropout=False, activ_name='lerelu'):\n",
        "        super(ExpandBlock, self).__init__()\n",
        "        self.layer1 = Seq()\n",
        "        self.layer1.add_module('conv', tranpose_by_name(kernel_size)(in_channels, out_channels, stride)),\n",
        "        if apply_dropout:\n",
        "            self.layer1.add_module('batch', nn.BatchNorm2d(out_channels)),\n",
        "        self.layer1.add_module('relu', activ_by_name(activ_name))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ynrzoAKdIlR"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        \n",
        "        self.layer1 = Seq(\n",
        "            CompressBlock(4, 64, '4x4', False),\n",
        "            CompressBlock(64, 128, '4x4'),\n",
        "            CompressBlock(128, 256, '4x4'),\n",
        "            nn.ZeroPad2d(1),\n",
        "            nn.Conv2d(256, 512, 4),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.ZeroPad2d(1),\n",
        "            nn.Conv2d(512, 1, 4),\n",
        "            \n",
        "            nn.Flatten(),\n",
        "            nn.Linear(900, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self, x, y):\n",
        "        x = torch.cat((y, x), dim=1)\n",
        "        \n",
        "        x = self.layer1(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "\n",
        "dis = Discriminator()\n",
        "img = torch.randn(8, 3, 256, 256)\n",
        "edge = torch.randn(8, 1, 256, 256)\n",
        "dis(img, edge).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxh8F4WXdKwv"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        \n",
        "        self.layer1 = nn.ModuleList([\n",
        "            CompressBlock(1, 64, '4x4', apply_batchnorm=False),\n",
        "            CompressBlock(64, 128, '4x4'),     #64\n",
        "            CompressBlock(128, 256, '4x4'),    #32\n",
        "            CompressBlock(256, 512, '4x4'),    #16\n",
        "            CompressBlock(512, 512, '4x4'),    #8\n",
        "            CompressBlock(512, 512, '4x4'),    #4\n",
        "            CompressBlock(512, 512, '4x4'),    #2\n",
        "        ])\n",
        "        self.layer2 = nn.ModuleList([\n",
        "            ExpandBlock(1024, 512, '3x3', apply_dropout=True),\n",
        "            ExpandBlock(1024, 512, '3x3', apply_dropout=True),  \n",
        "            ExpandBlock(1024, 512, '3x3', apply_dropout=True),  \n",
        "            ExpandBlock(1024, 256, '3x3'),  \n",
        "            ExpandBlock(512, 128, '3x3'),  \n",
        "            ExpandBlock(256, 64, '3x3'), \n",
        "            nn.ConvTranspose2d(128, 3, 4, 2, 1)\n",
        "        ])\n",
        "        self.tanh = nn.Tanh()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        skips = []\n",
        "        for layer in self.layer1:\n",
        "            x = layer(x)\n",
        "            skips.append(x)\n",
        "        \n",
        "        skips = reversed(skips)\n",
        "        for layer, skip in zip(self.layer2, skips):\n",
        "            x = torch.cat((x, skip), dim=1)\n",
        "            x = layer(x)\n",
        "        \n",
        "        x = self.tanh(x)\n",
        "        \n",
        "        return x\n",
        "    \n",
        "img = torch.randn(1, 1, 256, 256)\n",
        "gen = Generator()\n",
        "gen(img).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8-0bGeIdNZI"
      },
      "source": [
        "def log_gen_losses(title, losses, step):\n",
        "    title = f'{title}_gen_losses'\n",
        "\n",
        "    loss_names = ['loss_fake', 'loss_total']\n",
        "    \n",
        "    for loss, loss_name in zip(losses, loss_names):\n",
        "        wandb.log({f'{title}/{loss_name}': loss}, step=step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF7NJJmudP02"
      },
      "source": [
        "def log_dis_losses(title, losses, step):\n",
        "    title = f'{title}_dis_losses'\n",
        "\n",
        "    loss_names = ['loss_real', 'loss_fake', 'loss_total']\n",
        "    \n",
        "    for loss, loss_name in zip(losses, loss_names):\n",
        "        wandb.log({f'{title}/{loss_name}': loss}, step=step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkBZdnirdSgS"
      },
      "source": [
        "from torchvision.utils import make_grid\n",
        "\n",
        "@torch.no_grad()\n",
        "def log_images(title, name, x, step):\n",
        "    title = f'{title}_images'\n",
        "\n",
        "    imgs = make_grid(x, nrow=5)\n",
        "\n",
        "    wandb.log({f'{title}/{name}': [wandb.Image(imgs)]}, step=step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjXG96SxdXoG"
      },
      "source": [
        "def calc_gen_losses(gen, dis, x_real, criterion):\n",
        "\n",
        "    x_edge_raw = [to_edge(i) for i in x_real]\n",
        "    x_edge = torch.cat(x_edge_raw).to(device)\n",
        "    x_fake = gen(x_edge)\n",
        "    \n",
        "    out_fake = dis(x_fake, x_edge)\n",
        "\n",
        "    target_real = torch.ones_like(out_fake).to(device)\n",
        "\n",
        "    loss_fake = criterion(out_fake, target_real)\n",
        "\n",
        "    loss_total = loss_fake\n",
        "\n",
        "    losses = np.array([loss_fake.item(), loss_total.item()])\n",
        "\n",
        "    return losses, loss_total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72t8St6ydaf3"
      },
      "source": [
        "def calc_dis_losses(gen, dis, x_real, criterion):\n",
        "\n",
        "    x_edge_raw = [to_edge(i) for i in x_real]\n",
        "    x_edge = torch.cat(x_edge_raw).to(device)\n",
        "    \n",
        "    x_fake = gen(x_edge).detach()\n",
        "    \n",
        "    out_real = dis(x_real, x_edge)\n",
        "    out_fake = dis(x_fake, x_edge)\n",
        "\n",
        "    target_real = torch.ones_like(out_real).to(device)\n",
        "    target_fake = torch.zeros_like(out_fake).to(device)\n",
        "\n",
        "    loss_real = criterion(out_real, target_real)\n",
        "    loss_fake = criterion(out_fake, target_fake)\n",
        "\n",
        "    loss_total = loss_real + loss_fake\n",
        "\n",
        "    losses = np.array([loss_real.item(), loss_fake.item(), loss_total.item()])\n",
        "\n",
        "    return losses, loss_total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHn6TQilddA-"
      },
      "source": [
        "def train_step(gen, dis, x, criterion, optim_gen, optim_dis, step):\n",
        "    dis_losses, dis_loss_total = calc_dis_losses(gen, dis, x, criterion)\n",
        "\n",
        "    optim_dis.zero_grad()\n",
        "    dis_loss_total.backward()\n",
        "    optim_dis.step()\n",
        "\n",
        "    gen_losses, gen_loss_total = calc_gen_losses(gen, dis, x, criterion)\n",
        "\n",
        "    optim_gen.zero_grad()\n",
        "    gen_loss_total.backward()\n",
        "    optim_gen.step()\n",
        "    log_gen_losses('Train', gen_losses, step)\n",
        "    log_dis_losses('Train', dis_losses, step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax5b3c4udfug"
      },
      "source": [
        "from tqdm.auto import trange, tqdm\n",
        "\n",
        "\n",
        "def train_epoch(gen, dis, criterion, optim_gen, optim_dis, dataloader, num_save, step):\n",
        "    gen.train()\n",
        "    dis.train()\n",
        "\n",
        "    for x, _ in tqdm(dataloader):\n",
        "        x = x.to(device)\n",
        "\n",
        "        train_step(gen, dis, x, criterion, optim_gen, optim_dis, step)\n",
        "        \n",
        "        x_edge_raw = [to_edge(i) for i in x]\n",
        "        x_edge = torch.cat(x_edge_raw).to(device)\n",
        "\n",
        "        x_fake = gen(x_edge).detach()\n",
        "        \n",
        "        if step % 2000 == 0:\n",
        "            torch.save(gen, 'W_MODELS_OF_ARCHIBALD/GENERATOR_'+str(num_save))\n",
        "            torch.save(dis, 'W_MODELS_OF_ARCHIBALD/DISCRIMINATOR_'+str(num_save))\n",
        "            num_save += 1\n",
        "        \n",
        "        if step % 200 == 0:\n",
        "            log_images('Images', 'real', x, step)\n",
        "            log_images('Images', 'edge', x_edge, step)\n",
        "            log_images('Images', 'fake', x_fake, step)\n",
        "        step += 1\n",
        "\n",
        "    return step, num_save\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MOGRJmLdkR1"
      },
      "source": [
        "def train(gen, dis, criterion, optim_gen, optim_dis, trainloader, valloader, epochs):\n",
        "    step = 0\n",
        "    num_save = 1\n",
        "    for epoch in range(epochs):\n",
        "        step, num_save = train_epoch(gen, dis, criterion, optim_gen, optim_dis, trainloader, num_save, step)\n",
        "    \n",
        "        wandb.log({'epoch': epoch}, step=step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82TqqyBCdpoa"
      },
      "source": [
        "gen = Generator()\n",
        "dis = Discriminator()\n",
        "gen.to(device)\n",
        "dis.to(device)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "optim_gen = torch.optim.Adam(gen.parameters(), lr=0.0003, betas=(0.5, 0.999))\n",
        "optim_dis = torch.optim.Adam(dis.parameters(), lr=0.0003, betas=(0.5, 0.999))\n",
        "\n",
        "wandb.init(project='PIX_2_PIX_HYPE')\n",
        "\n",
        "train(gen, dis, criterion, optim_gen, optim_dis, trainloader, valloader, epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EY-rAXTdsvi"
      },
      "source": [
        "iter(trainloader).next()[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYmIeWahdtYs"
      },
      "source": [
        "iter(trainloader).next()[0].apply_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d91pSzsOdxzB"
      },
      "source": [
        "to_edge(iter(trainloader).next()[0][0]).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwemqn2Ed3O9"
      },
      "source": [
        "torch.save(gen, 'MODELS_OF_ARCHIBALD/GENERATOR_'+str(0))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}